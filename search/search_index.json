{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Next generation sequencing and bioinformatic and genomic analysis at CDPHE is not CLIA validated at this time. These workflows and their outputs are not to be used for diagnostic purposes and should only be used for public health action and surveillance purposes. CDPHE is not responsible for the incorrect or inappropriate use of these workflows or their results. The following documentation describes the Colorado Department of Public Health and Environments's (CDPHE) workflows for the assembly and analysis of whole genome sequencing data of SARS-CoV-2 on GCP's Terra.bio platform. Workflows are written in WDL and can be imported into a Terra.bio workspace through dockstore. Our SARS-CoV-2 whole genome reference-based assembly workflows are highly adaptable and facilitate the assembly and analysis of tiled amplicon based sequencing data of SARS-CoV-2. The workflows can accomodate various amplicon primer schemes including Artic V3, Artic V4, Artic V4.1 and Midnight, as well as diffent sequencing technology platforms including both Illumina and Oxford Nanopore Technology (ONT). Below is a high level overview of our workflows followed by detailed descriptions of each workflow which you can access by clicking the dropdown menus. Briefly, we begin with one of our processing python scripts (see ''../preprocess_python_scripts'' for more details), which organizes raw fastq files from either Illumina or ONT platforms, pushes the reads to a specified google bucket, and generates an input data table for Terra.bio. Next, using the platform appropriate assembly workflow on Terra.bio ( SC2_illumina_pe_assembly , SC2_illumina_se_assembly , or SC2_ont_assembly ), we perform quality control, trimming, and filtering of raw reads, and perform reference-guided whole genome assembly. Following assembly, intermediate and results files are transfered to a user defined google storage bucket using the appropriate transfer workflow ( SC2_transfer_illumina_pe_assembly , SC2_transfer_illumina_se_assembly , or SC2_transfer_ont_assembly ). Next, we use Pangolin and Nextclade to peform clade and lineage assignment on the consesnus assemblies and produce a results summary file for the set of sequences analyzed using the SC2_lineage_calling_and_results workflow. If you already have a multifasta, you can use the SC2_multifasta_lineage_calling workflow for clade and lineage assignment. We genearte a nextstrain build usig the publically available Nextstrain workflow . For wastewater samples, bam files generated from one of the three assembly workflows can be used as input in our SC2_wastewater_variant_calling workflow . Below is a high level overview of our workflow process that gets us from fastq files to lineage calls. As of Jan 2023, SC2_illumina_se_assembly is no longer maintained. This workflow was developed for the assembly of Illumina 72 bp single-end read data using the Illumina COVIDSEQ library prep protocol. We no longer use this library prep method.","title":"Home"},{"location":"#overview","text":"Next generation sequencing and bioinformatic and genomic analysis at CDPHE is not CLIA validated at this time. These workflows and their outputs are not to be used for diagnostic purposes and should only be used for public health action and surveillance purposes. CDPHE is not responsible for the incorrect or inappropriate use of these workflows or their results. The following documentation describes the Colorado Department of Public Health and Environments's (CDPHE) workflows for the assembly and analysis of whole genome sequencing data of SARS-CoV-2 on GCP's Terra.bio platform. Workflows are written in WDL and can be imported into a Terra.bio workspace through dockstore. Our SARS-CoV-2 whole genome reference-based assembly workflows are highly adaptable and facilitate the assembly and analysis of tiled amplicon based sequencing data of SARS-CoV-2. The workflows can accomodate various amplicon primer schemes including Artic V3, Artic V4, Artic V4.1 and Midnight, as well as diffent sequencing technology platforms including both Illumina and Oxford Nanopore Technology (ONT). Below is a high level overview of our workflows followed by detailed descriptions of each workflow which you can access by clicking the dropdown menus. Briefly, we begin with one of our processing python scripts (see ''../preprocess_python_scripts'' for more details), which organizes raw fastq files from either Illumina or ONT platforms, pushes the reads to a specified google bucket, and generates an input data table for Terra.bio. Next, using the platform appropriate assembly workflow on Terra.bio ( SC2_illumina_pe_assembly , SC2_illumina_se_assembly , or SC2_ont_assembly ), we perform quality control, trimming, and filtering of raw reads, and perform reference-guided whole genome assembly. Following assembly, intermediate and results files are transfered to a user defined google storage bucket using the appropriate transfer workflow ( SC2_transfer_illumina_pe_assembly , SC2_transfer_illumina_se_assembly , or SC2_transfer_ont_assembly ). Next, we use Pangolin and Nextclade to peform clade and lineage assignment on the consesnus assemblies and produce a results summary file for the set of sequences analyzed using the SC2_lineage_calling_and_results workflow. If you already have a multifasta, you can use the SC2_multifasta_lineage_calling workflow for clade and lineage assignment. We genearte a nextstrain build usig the publically available Nextstrain workflow . For wastewater samples, bam files generated from one of the three assembly workflows can be used as input in our SC2_wastewater_variant_calling workflow . Below is a high level overview of our workflow process that gets us from fastq files to lineage calls. As of Jan 2023, SC2_illumina_se_assembly is no longer maintained. This workflow was developed for the assembly of Illumina 72 bp single-end read data using the Illumina COVIDSEQ library prep protocol. We no longer use this library prep method.","title":"Overview"},{"location":"assembly_workflow/","text":"Assembly Workflows The following three workflows describe the reference based assembly methods for paired-end and single end illumina seuqencing data and ONT sequencing data. Each workflow accepts \"sample\" as the root entity type. Illumina PE File: SC2_illumina_pe_assembly.wdl This workflow was developed for the assembly of Illumina 150 bp paired-end read data using the Illumina Nextera XT library prep protocol. The workflow accepts \"sample\" as the root entity type. The workflow will: Use Seqyclean to quality filter and trim raw fastq files Seqyclean parameters include a minimum read length set to 70 bp and quality trimming set to a minimum Phred quality score of 30. Run FastQC on both the raw and cleaned reads Align reads to the reference genome using bwa and then sort the bam by coordinates using Samtools Use iVar trim to trim primer regions and then sort the trimmed bam by coordinates using Samtools Use iVar variants to call variants from the trimmed and sorted bam iVar variants parameters include a minimum quality score set to 20, a minimum variant base frequency set to 0.6 and a minimum read depth set to 10. Use iVar consensus to call the consensus genome sequence from the trimmed and sorted bam iVar consensus parameters include a minimum quality score set to 20, a minimum variant base frequency set to 0.6 and a minimum read depth set to 10. Use Samtools flagstat, stats, and coverage to output statistics from the bam Rename the fasta header of consensus sequences in the GISAID-acceptable format: CO-CDPHE-{sample_id} Calculate the percent coverage using the calc_percent_coverage.py script available in the python_scripts directory of this repo. Inputs 1. Terra Data Table The terra data table must include the following columns as listed below. Note that optional columns are not neccessary for the assembly workflow but must be present for the SC2_lineage_calling_and results.wdl and Transfer workflows described below under Lineage Calling Workflows and Transfer Workflows , respecitively. column header description entity:sample_id column with the list of sample names. (e.g. entity:covwwt-0203_id ) fastq_1 The google bucket path to the R1 fastq file. fastq_2 The google bucket path to the R2 fastq file. out_dir User defined google bucket for where the files will be transfered during the transfer workflows. workbook_path (optional; required for lineage calling workflow) project_name (optional; requried for lineage calling workflow) 2. Terra Workspace Data See setup . 3. Setting Up the Workflow Inputs For setting up the worklfow inputs, use the SC2_illumina_pe_assembly-input.json in the workflow_inputs directory. workflow variable attribute (input syntax into workflow) adapters_and_contaminants workspace.adapters_and_contaminants_fa calc_percent_coverage_py workspace.covid_calc_percent_coverage_py covid_genome workspace.covid_genome_fa covid_gff workspace.covid_genome_gff fastq_1 this.fastq_1 fastq_2 this.fastq_2 primer_bed workspace.artic_v4-1_bed s_gene_amplicons workspace.artic_v4-1_s_gene_amplicons sample_name this.{entity_name}_id Outputs WDL task name software/program variable name description seqyclean seqyclean filtered_reads_1 file seqyclean seqyclean filtered_reads_2 file seqyclean seqyclean seqyclean_summary file fastqc as fastqc_raw fastqc fastqc_raw1_html file fastqc as fastqc_raw fastqc fastqc_raw1_zip file fastqc as fastqc_raw fastqc fastqc_raw2_html file fastqc as fastqc_raw fastqc fastqc_raw2_zip file fastqc as fastqc_cleaned fastqc fastqc_clean1_html file fastqc as fastqc_cleaned fastqc fastqc_clean1_zip file fastqc as fastqc_cleaned fastqc fastqc_clean2_html file fastqc as fastqc_cleaned fastqc fastqc_clean2_zip file align_reads bwa and samtools out_bam file align_reads bwa and samtools out_bamindex file align_reads bwa and samtools assembler_version string recording the version for bwa, this information is used later for submitting to public repositories. ivar trim ivar trim and samtools trim_bam file ivar trim ivar trim and samtools trimsort_bam file ivar trim ivar trim and samtools trimsort_bamindex file ivar variants ivar variants variants vcf file formatted as a tsv ivar consensus ivar consnesus consensus fasta file of consensus genome, Ns are called in places with less than 10 bp read depth. bam_stats samtools flagstat, stats, percent_coverage flagstat_out file bam_stats samtools flagstat, stats, percent_coverage stats_out file bam_stats samtools flagstat, stats, percent_coverage covhist_out file bam_stats samtools flagstat, stats, percent_coverage cov_out file bam_stats samtools flagstat, stats, percent_coverage cov_s_gene_amplcions_out file bam_stats samtools flagstat, stats, percent_coverage cov_s_gene_out file rename_fasta N/A renamed_consensus fasta file; consensus genome sequence with the fasta header renamed to be CO-CDPHE-{sample_name} calc_percent_cvg calc_percent_coverage.py percent_cvg_csv csv file, see calc_percent_cvg.py script readme for details found in the ./python_scripts directory of this repository. Illumina SE This workflow is no longer maintained. File: SC2_illumina_se_assembly.wdl This workflow was developed for the assembly of Illumina 72 bp single-end read data using the Illumina COVIDSEQ library prep protocol. The workflow accepts \"sample\" as the root entity type. The workflow will: Use Trimmomatic and bbduk to quality filter, trim, and remove adapters from raw fastq files Trimmomatic parameters inlcude a sliding widnow set to trim reads when the 4bp sliding window quality score falls below a mean Phred quality score of 30 (i.e. 4:30) and a minimum read lenght of 25 bp. bbduck parameters include adapter trimming set to trim everything to the right of a kmer match and removal of PhiX sequences. Run FastQC on both the raw and cleaned reads Align reads to the reference genome using bwa and then sort the bam by coordinates using Samtools Use iVar trim to trim primer regions and then sort the trimmed bam by coordinates using Samtools Use iVar variants to call variants from the trimmed and sorted bam iVar variants parameters include a minimum quality score set to 20, a minimum variant base frequency set to 0.6 and a minimum read depth set to 10. Use iVar consensus to call the consensus genome sequence from the trimmed and sorted bam iVar consensus parameters include a minimum quality score set to 20, a minimum variant base frequency set to 0.6 and a minimum read depth set to 10. Use Samtools flagstat, stats, and coverage to output statistics from the bam Rename the fasta header of consensus sequences in the format: CO-CDPHE{sample_id} Calculate the percent coverage using the calc_percent_coverage.py script available in the python_scripts directory of this repo. Inputs 1. Terra Data Table The terra data table can be generated using the preprocess python scripts available in the data preprocessing repository . The terra data table must include the following columns as listed below. Note that optional columns are not neccessary for the assembly workflow but be present for the SC2_lineage_calling_and results.wdl and Transfer workflows described below under Lineage Calling Workflows and Transfer Workflows , respectively. entity:sample_id : column with the list of sample names/ids. Note that if there is more than one data table in the Terra Workspace, you need to add a number after the word sample to keep the datatables seperate (e.g. entity:sample2_id ). fastq : The google bucket path to the fastq file. seq_run (optional): the name of the sequencing run (e.g. NEXSEQ_101) tech_platform (optional) : e.g. Illumina NexSeq read_type (optional): single primer_set (optional): e.g. COVIDSeqV3 plate_name (optional): name of sequencing plate plate_sample_well (optional): location of well on sequencing plate out_dir (optional): user defined google bucket fro where the files will be transfered during the transfer workflows. 2. Terra Workspace Data The following reference files can be found in the workspace_data directory and the python_scripts directory. These files should be saved as Workspace data in your Terra Workspace. To do so, upload the files to a google bucket an link the file path to the wrokspace data variable. Once saved as workspace data variables, they can be used as inputs for the workflow. covid_genome : the path to the google bucket directory contianing the SARS-CoV-2 reference genome fasta (we use NCBI genbank ID MN908947.3). covid_gff : the path to the google bucket directory containing the SARS-CoV-2 reference genome gff annotation file (we use NCBI genbank ID MN908947.3) primer_bed : the path to the google bucket directory containing a bed file with the primers used for amplicon sequencing currenly we have bed files for Artic V3, Artic V4, Artic V4.1 and Midnight. preprocess_python_script : [do we want to change the name of this variable in the WDL to match the python script name?] the path to the google bucket containing the calc_percent_coverage.py script. Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.\" syntax refers Terra to pull the variable from the terra datatable (#1 above). The \"workspace.\" syntax refers Terra to pull the variable from the terra workspace data (#2 above). workflow variable attribute (input syntax into workflow) covid_genome workspace.covid_genome covid_gff workspace.covid_gff fastq this.fastq preprocess_python_script workspace.preprocess_python_script primer_bed workspace.V4-1Artic sample_id this.sample{terra_datatable_name}_id Outputs 1. Output Files from Trimmomatic and ddbuk trimmed_reads : file trim_stats : file filtered_reads : file adapter_stats : file PhiX_stats : file 2. Output Files from FastQC fastqc_raw1_html : file fastqc_raw1_zip : file fastqc_raw2_html : file fastqc_raw2_zip : file fastqc_clean1_html : file fastqc_clean1_zip : file fastqc_clean2_html : file fastqc_clean2_zip : file 3. Output files from bwa and samtools (align reads) out_bam : file 4. Output files from iVar trim and samtools trim_bam : file trimsort_bam : file trimsort_bamindex : file 5. Output files from iVar variants variants : vcf file formated as a tsv 6. Output files from iVar consensus consensus : fasta file of conensus genome, Ns are called in places with less than 10 bp read depth. 7. Output files from Samtools flagstat, stats, and percent_coverage fagstat_out : file stats_out : file covhist_out : file cov_out : file 8. Output from rename consensus fasta headers renamed_consensus : fasta file; consesnus genome sequence with the fasta header renamed to be CO-CDPHE-{sample_id} 9. Output from calc_percent_coverage.py percent_cvg_csv : csv file, see calc_percent_cvg.py script readme for details. 10. bwa assembler version string output assembler_version : string recording the version for bwa, this information is used later for submitting to public repositories. Oxford Nanopore Technologies (ONT) File: SC2_ont_assembly.wdl This workflow was developed for the assembly of Oxford Nanopore Technology (ONT) read data following the ARTIC SARS-CoV-2 sequencing protocol and using the ONT native barcoding kit. This workflow assumes that basecalling and conversion of fast5 files into fastq has already occurred (e.g. using MinKNOW). The workflow accepts \"sample\" as the root entity type. The workflow will: Demuliplex basecalled fastq files using guppy_barcoder Perform quality filering using guppyplex guppyplex inlcudes a min length parameter set to 400 and a max length set to 700 for Artic primers and a min lingth set ot 400 and a max length set to 1500 for midnight primers. Run artic minion --medaka for variant calling and to generate a consensus fagstat_out medaka uses minimap2 by default to align reads to the SARS-CoV-2 reference genome the default parameter in medaka for base calling is 20x depth and at least 60% of reads containing the base call Scaffold assembly with pyScaf this step ensures a single continuous consensus sequence with only one sequence in the consensus fasta file Rename consensus to CO-CDPHE-{sample_id} Generate bam quality statistics using samtools Calculates percent coverage using the calc_percent_coverage.py script Inputs 1. Terra Data Table The terra data table can be generated using the preprocess python scripts available in the data preprocessing repository . The terra data table must include the following columns as listed below. Note that optional columns are not neccessary for the assembly workflow but but be present for the SC2_lineage_calling_and results.wdl and Transfer workflows described below under Lineage Calling Workflows and Transfer Workflows , respectively. column header description entity:sample_id column with the list of sample names. (e.g. entity:covwwt-0203_id ) index_1_id the ont barcode associated with the sample fastq_dir the google bucket path with the set of fastq files out_dir User defined google bucket for where the files will be transfered during the transfer workflows. workbook_path (optional; required for lineage calling workflow) project_name (optional; requried for lineage calling workflow) 2. Terra Workspace Data See setup . 3. Setting Up the Workflow Inputs For setting up the worklfow inputs, use the SC2_ont_assembly-input.json in the workflow_inputs directory. workflow variable attribute (input syntax into workflow) calc_percent_coverage_py workspace.covid_calc_percent_coverage_py covid_genome workspace.covid_genome_fa gcs_fastq_dir this.fastq_dir index_1_id this.index_1_id primer_bed workspace.artic_v4-1_bed primer_set this.primer_set s_gene_amplicons workspace.artic_v4-1_s_gene_amplicons s_gene_primer_bed workspace.artic_v4-1_s_gene_primer_bed sample_name this.{entity_name}_id Outputs WDL task name software/program variable name description Demultiplex guppy_barcoder barcode_summary file Demultiplex guppy_barcoder guppy_dmux_fastq file guppyplex quality filtering filtered_fatsq file Medaka medaka and minimap2 sorted_bam file Medaka medaka and minimap2 trim_sort_bam file Medaka medaka and minimap2 trimsort_bai file Medaka medaka and minimap2 variants file Medaka medaka and minimap2 consensus file Medaka medaka and minimap2 assembler_version string recording the version for artic medaka, this information is used later for submitting to public repositories. Bam_stats samtools flagstat, stats, percent_coverage flagstat_out file Bam_stats samtools flagstat, stats, percent_coverage stats_out file Bam_stats samtools flagstat, stats, percent_coverage covhist_out file Bam_stats samtools flagstat, stats, percent_coverage cov_out file Scaffold pyScaf scaffold_consensus consesnus sequence as a fasta file rename_fasta N/A renamed_consensus fasta file; consesnus genome sequence with the fasta header renamed to be CO-CDPHE-{sample_name} calc_percent_cvg calc_percent_coverage.py percent_cvg_csv csv file, see calc_percent_cvg.py script readme for details found in the ./python_scripts directory of this repository. get_primer_site_variants bcftools primer_site_variants file","title":"Assembly Workflows"},{"location":"assembly_workflow/#assembly-workflows","text":"The following three workflows describe the reference based assembly methods for paired-end and single end illumina seuqencing data and ONT sequencing data. Each workflow accepts \"sample\" as the root entity type.","title":"Assembly Workflows"},{"location":"assembly_workflow/#illumina-pe","text":"File: SC2_illumina_pe_assembly.wdl This workflow was developed for the assembly of Illumina 150 bp paired-end read data using the Illumina Nextera XT library prep protocol. The workflow accepts \"sample\" as the root entity type. The workflow will: Use Seqyclean to quality filter and trim raw fastq files Seqyclean parameters include a minimum read length set to 70 bp and quality trimming set to a minimum Phred quality score of 30. Run FastQC on both the raw and cleaned reads Align reads to the reference genome using bwa and then sort the bam by coordinates using Samtools Use iVar trim to trim primer regions and then sort the trimmed bam by coordinates using Samtools Use iVar variants to call variants from the trimmed and sorted bam iVar variants parameters include a minimum quality score set to 20, a minimum variant base frequency set to 0.6 and a minimum read depth set to 10. Use iVar consensus to call the consensus genome sequence from the trimmed and sorted bam iVar consensus parameters include a minimum quality score set to 20, a minimum variant base frequency set to 0.6 and a minimum read depth set to 10. Use Samtools flagstat, stats, and coverage to output statistics from the bam Rename the fasta header of consensus sequences in the GISAID-acceptable format: CO-CDPHE-{sample_id} Calculate the percent coverage using the calc_percent_coverage.py script available in the python_scripts directory of this repo.","title":"Illumina PE"},{"location":"assembly_workflow/#inputs","text":"","title":"Inputs"},{"location":"assembly_workflow/#1-terra-data-table","text":"The terra data table must include the following columns as listed below. Note that optional columns are not neccessary for the assembly workflow but must be present for the SC2_lineage_calling_and results.wdl and Transfer workflows described below under Lineage Calling Workflows and Transfer Workflows , respecitively. column header description entity:sample_id column with the list of sample names. (e.g. entity:covwwt-0203_id ) fastq_1 The google bucket path to the R1 fastq file. fastq_2 The google bucket path to the R2 fastq file. out_dir User defined google bucket for where the files will be transfered during the transfer workflows. workbook_path (optional; required for lineage calling workflow) project_name (optional; requried for lineage calling workflow)","title":"1. Terra Data Table"},{"location":"assembly_workflow/#2-terra-workspace-data","text":"See setup .","title":"2. Terra Workspace Data"},{"location":"assembly_workflow/#3-setting-up-the-workflow-inputs","text":"For setting up the worklfow inputs, use the SC2_illumina_pe_assembly-input.json in the workflow_inputs directory. workflow variable attribute (input syntax into workflow) adapters_and_contaminants workspace.adapters_and_contaminants_fa calc_percent_coverage_py workspace.covid_calc_percent_coverage_py covid_genome workspace.covid_genome_fa covid_gff workspace.covid_genome_gff fastq_1 this.fastq_1 fastq_2 this.fastq_2 primer_bed workspace.artic_v4-1_bed s_gene_amplicons workspace.artic_v4-1_s_gene_amplicons sample_name this.{entity_name}_id","title":"3. Setting Up the Workflow Inputs"},{"location":"assembly_workflow/#outputs","text":"WDL task name software/program variable name description seqyclean seqyclean filtered_reads_1 file seqyclean seqyclean filtered_reads_2 file seqyclean seqyclean seqyclean_summary file fastqc as fastqc_raw fastqc fastqc_raw1_html file fastqc as fastqc_raw fastqc fastqc_raw1_zip file fastqc as fastqc_raw fastqc fastqc_raw2_html file fastqc as fastqc_raw fastqc fastqc_raw2_zip file fastqc as fastqc_cleaned fastqc fastqc_clean1_html file fastqc as fastqc_cleaned fastqc fastqc_clean1_zip file fastqc as fastqc_cleaned fastqc fastqc_clean2_html file fastqc as fastqc_cleaned fastqc fastqc_clean2_zip file align_reads bwa and samtools out_bam file align_reads bwa and samtools out_bamindex file align_reads bwa and samtools assembler_version string recording the version for bwa, this information is used later for submitting to public repositories. ivar trim ivar trim and samtools trim_bam file ivar trim ivar trim and samtools trimsort_bam file ivar trim ivar trim and samtools trimsort_bamindex file ivar variants ivar variants variants vcf file formatted as a tsv ivar consensus ivar consnesus consensus fasta file of consensus genome, Ns are called in places with less than 10 bp read depth. bam_stats samtools flagstat, stats, percent_coverage flagstat_out file bam_stats samtools flagstat, stats, percent_coverage stats_out file bam_stats samtools flagstat, stats, percent_coverage covhist_out file bam_stats samtools flagstat, stats, percent_coverage cov_out file bam_stats samtools flagstat, stats, percent_coverage cov_s_gene_amplcions_out file bam_stats samtools flagstat, stats, percent_coverage cov_s_gene_out file rename_fasta N/A renamed_consensus fasta file; consensus genome sequence with the fasta header renamed to be CO-CDPHE-{sample_name} calc_percent_cvg calc_percent_coverage.py percent_cvg_csv csv file, see calc_percent_cvg.py script readme for details found in the ./python_scripts directory of this repository.","title":"Outputs"},{"location":"assembly_workflow/#illumina-se","text":"This workflow is no longer maintained. File: SC2_illumina_se_assembly.wdl This workflow was developed for the assembly of Illumina 72 bp single-end read data using the Illumina COVIDSEQ library prep protocol. The workflow accepts \"sample\" as the root entity type. The workflow will: Use Trimmomatic and bbduk to quality filter, trim, and remove adapters from raw fastq files Trimmomatic parameters inlcude a sliding widnow set to trim reads when the 4bp sliding window quality score falls below a mean Phred quality score of 30 (i.e. 4:30) and a minimum read lenght of 25 bp. bbduck parameters include adapter trimming set to trim everything to the right of a kmer match and removal of PhiX sequences. Run FastQC on both the raw and cleaned reads Align reads to the reference genome using bwa and then sort the bam by coordinates using Samtools Use iVar trim to trim primer regions and then sort the trimmed bam by coordinates using Samtools Use iVar variants to call variants from the trimmed and sorted bam iVar variants parameters include a minimum quality score set to 20, a minimum variant base frequency set to 0.6 and a minimum read depth set to 10. Use iVar consensus to call the consensus genome sequence from the trimmed and sorted bam iVar consensus parameters include a minimum quality score set to 20, a minimum variant base frequency set to 0.6 and a minimum read depth set to 10. Use Samtools flagstat, stats, and coverage to output statistics from the bam Rename the fasta header of consensus sequences in the format: CO-CDPHE{sample_id} Calculate the percent coverage using the calc_percent_coverage.py script available in the python_scripts directory of this repo.","title":"Illumina SE"},{"location":"assembly_workflow/#inputs_1","text":"","title":"Inputs"},{"location":"assembly_workflow/#1-terra-data-table_1","text":"The terra data table can be generated using the preprocess python scripts available in the data preprocessing repository . The terra data table must include the following columns as listed below. Note that optional columns are not neccessary for the assembly workflow but be present for the SC2_lineage_calling_and results.wdl and Transfer workflows described below under Lineage Calling Workflows and Transfer Workflows , respectively. entity:sample_id : column with the list of sample names/ids. Note that if there is more than one data table in the Terra Workspace, you need to add a number after the word sample to keep the datatables seperate (e.g. entity:sample2_id ). fastq : The google bucket path to the fastq file. seq_run (optional): the name of the sequencing run (e.g. NEXSEQ_101) tech_platform (optional) : e.g. Illumina NexSeq read_type (optional): single primer_set (optional): e.g. COVIDSeqV3 plate_name (optional): name of sequencing plate plate_sample_well (optional): location of well on sequencing plate out_dir (optional): user defined google bucket fro where the files will be transfered during the transfer workflows.","title":"1. Terra Data Table"},{"location":"assembly_workflow/#2-terra-workspace-data_1","text":"The following reference files can be found in the workspace_data directory and the python_scripts directory. These files should be saved as Workspace data in your Terra Workspace. To do so, upload the files to a google bucket an link the file path to the wrokspace data variable. Once saved as workspace data variables, they can be used as inputs for the workflow. covid_genome : the path to the google bucket directory contianing the SARS-CoV-2 reference genome fasta (we use NCBI genbank ID MN908947.3). covid_gff : the path to the google bucket directory containing the SARS-CoV-2 reference genome gff annotation file (we use NCBI genbank ID MN908947.3) primer_bed : the path to the google bucket directory containing a bed file with the primers used for amplicon sequencing currenly we have bed files for Artic V3, Artic V4, Artic V4.1 and Midnight. preprocess_python_script : [do we want to change the name of this variable in the WDL to match the python script name?] the path to the google bucket containing the calc_percent_coverage.py script. Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.\" syntax refers Terra to pull the variable from the terra datatable (#1 above). The \"workspace.\" syntax refers Terra to pull the variable from the terra workspace data (#2 above). workflow variable attribute (input syntax into workflow) covid_genome workspace.covid_genome covid_gff workspace.covid_gff fastq this.fastq preprocess_python_script workspace.preprocess_python_script primer_bed workspace.V4-1Artic sample_id this.sample{terra_datatable_name}_id","title":"2. Terra Workspace Data"},{"location":"assembly_workflow/#outputs_1","text":"","title":"Outputs"},{"location":"assembly_workflow/#1-output-files-from-trimmomatic-and-ddbuk","text":"trimmed_reads : file trim_stats : file filtered_reads : file adapter_stats : file PhiX_stats : file","title":"1. Output Files from Trimmomatic and ddbuk"},{"location":"assembly_workflow/#2-output-files-from-fastqc","text":"fastqc_raw1_html : file fastqc_raw1_zip : file fastqc_raw2_html : file fastqc_raw2_zip : file fastqc_clean1_html : file fastqc_clean1_zip : file fastqc_clean2_html : file fastqc_clean2_zip : file","title":"2. Output Files from FastQC"},{"location":"assembly_workflow/#3-output-files-from-bwa-and-samtools-align-reads","text":"out_bam : file","title":"3. Output files from bwa and samtools (align reads)"},{"location":"assembly_workflow/#4-output-files-from-ivar-trim-and-samtools","text":"trim_bam : file trimsort_bam : file trimsort_bamindex : file","title":"4. Output files from iVar trim and samtools"},{"location":"assembly_workflow/#5-output-files-from-ivar-variants","text":"variants : vcf file formated as a tsv","title":"5. Output files from iVar variants"},{"location":"assembly_workflow/#6-output-files-from-ivar-consensus","text":"consensus : fasta file of conensus genome, Ns are called in places with less than 10 bp read depth.","title":"6. Output files from iVar consensus"},{"location":"assembly_workflow/#7-output-files-from-samtools-flagstat-stats-and-percent_coverage","text":"fagstat_out : file stats_out : file covhist_out : file cov_out : file","title":"7. Output files from Samtools flagstat, stats, and percent_coverage"},{"location":"assembly_workflow/#8-output-from-rename-consensus-fasta-headers","text":"renamed_consensus : fasta file; consesnus genome sequence with the fasta header renamed to be CO-CDPHE-{sample_id}","title":"8. Output from rename consensus fasta headers"},{"location":"assembly_workflow/#9-output-from-calc_percent_coveragepy","text":"percent_cvg_csv : csv file, see calc_percent_cvg.py script readme for details.","title":"9. Output from calc_percent_coverage.py"},{"location":"assembly_workflow/#10-bwa-assembler-version-string-output","text":"assembler_version : string recording the version for bwa, this information is used later for submitting to public repositories.","title":"10. bwa assembler version string output"},{"location":"assembly_workflow/#oxford-nanopore-technologies-ont","text":"File: SC2_ont_assembly.wdl This workflow was developed for the assembly of Oxford Nanopore Technology (ONT) read data following the ARTIC SARS-CoV-2 sequencing protocol and using the ONT native barcoding kit. This workflow assumes that basecalling and conversion of fast5 files into fastq has already occurred (e.g. using MinKNOW). The workflow accepts \"sample\" as the root entity type. The workflow will: Demuliplex basecalled fastq files using guppy_barcoder Perform quality filering using guppyplex guppyplex inlcudes a min length parameter set to 400 and a max length set to 700 for Artic primers and a min lingth set ot 400 and a max length set to 1500 for midnight primers. Run artic minion --medaka for variant calling and to generate a consensus fagstat_out medaka uses minimap2 by default to align reads to the SARS-CoV-2 reference genome the default parameter in medaka for base calling is 20x depth and at least 60% of reads containing the base call Scaffold assembly with pyScaf this step ensures a single continuous consensus sequence with only one sequence in the consensus fasta file Rename consensus to CO-CDPHE-{sample_id} Generate bam quality statistics using samtools Calculates percent coverage using the calc_percent_coverage.py script","title":"Oxford Nanopore Technologies (ONT)"},{"location":"assembly_workflow/#inputs_2","text":"","title":"Inputs"},{"location":"assembly_workflow/#1-terra-data-table_2","text":"The terra data table can be generated using the preprocess python scripts available in the data preprocessing repository . The terra data table must include the following columns as listed below. Note that optional columns are not neccessary for the assembly workflow but but be present for the SC2_lineage_calling_and results.wdl and Transfer workflows described below under Lineage Calling Workflows and Transfer Workflows , respectively. column header description entity:sample_id column with the list of sample names. (e.g. entity:covwwt-0203_id ) index_1_id the ont barcode associated with the sample fastq_dir the google bucket path with the set of fastq files out_dir User defined google bucket for where the files will be transfered during the transfer workflows. workbook_path (optional; required for lineage calling workflow) project_name (optional; requried for lineage calling workflow)","title":"1. Terra Data Table"},{"location":"assembly_workflow/#2-terra-workspace-data_2","text":"See setup .","title":"2. Terra Workspace Data"},{"location":"assembly_workflow/#3-setting-up-the-workflow-inputs_1","text":"For setting up the worklfow inputs, use the SC2_ont_assembly-input.json in the workflow_inputs directory. workflow variable attribute (input syntax into workflow) calc_percent_coverage_py workspace.covid_calc_percent_coverage_py covid_genome workspace.covid_genome_fa gcs_fastq_dir this.fastq_dir index_1_id this.index_1_id primer_bed workspace.artic_v4-1_bed primer_set this.primer_set s_gene_amplicons workspace.artic_v4-1_s_gene_amplicons s_gene_primer_bed workspace.artic_v4-1_s_gene_primer_bed sample_name this.{entity_name}_id","title":"3. Setting Up the Workflow Inputs"},{"location":"assembly_workflow/#outputs_2","text":"WDL task name software/program variable name description Demultiplex guppy_barcoder barcode_summary file Demultiplex guppy_barcoder guppy_dmux_fastq file guppyplex quality filtering filtered_fatsq file Medaka medaka and minimap2 sorted_bam file Medaka medaka and minimap2 trim_sort_bam file Medaka medaka and minimap2 trimsort_bai file Medaka medaka and minimap2 variants file Medaka medaka and minimap2 consensus file Medaka medaka and minimap2 assembler_version string recording the version for artic medaka, this information is used later for submitting to public repositories. Bam_stats samtools flagstat, stats, percent_coverage flagstat_out file Bam_stats samtools flagstat, stats, percent_coverage stats_out file Bam_stats samtools flagstat, stats, percent_coverage covhist_out file Bam_stats samtools flagstat, stats, percent_coverage cov_out file Scaffold pyScaf scaffold_consensus consesnus sequence as a fasta file rename_fasta N/A renamed_consensus fasta file; consesnus genome sequence with the fasta header renamed to be CO-CDPHE-{sample_name} calc_percent_cvg calc_percent_coverage.py percent_cvg_csv csv file, see calc_percent_cvg.py script readme for details found in the ./python_scripts directory of this repository. get_primer_site_variants bcftools primer_site_variants file","title":"Outputs"},{"location":"nextstrain_workflow/","text":"Nextstrain Workflow We use the publicly available Nexstrain workflow to generate Nextstrain builds, and then transfer the results using this transfer workflow. This workflow transfers the output file generated from the publicly available sarscov2_nextstrain workflow to a user specified google bucket. Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. The Google Bucket path describes where in the user google bucket the output file is transferred to. Inputs workflow variable attribute (input syntax into workflow) google bucket path auspice_input_json this.auspice_input_json gs://{user_defined_gcp_bucket}/auspice_input_json/ combined_assemblies this.combined_assemblies gs://{user_defined_gcp_bucket}/combined_assemblies/ keep_list this.keep_list gs://{user_defined_gcp_bucket}/keep_list/ metadata_merged this.metadata gs://{user_defined_gcp_bucket}/metadata_merged/ ml_tree this.ml_tree gs://{user_defined_gcp_bucket}/ml_tree/ multiple_alignment this.multiple_alignment gs://{user_defined_gcp_bucket}/multiple_alignment/ node_data_jsons this.node_data_jsons gs://{user_defined_gcp_bucket}/node_data_jsons/ root_sequence_json this.root_sequence_json gs://{user_defined_gcp_bucket}/root_sequence_json/ subsampled_sequences this.subsampled_sequences gs://{user_defined_gcp_bucket}/subsampled_sequences/ time_tree this.time_tree gs://{user_defined_gcp_bucket}/time_tree/ tip_frequencies_json this.tip_frequencies_json gs://{user_defined_gcp_bucket}/tip_frequencies_json/ unmasked_snps this.unmasked_snps gs://{user_defined_gcp_bucket}/unmasked_snps/ out_dir gs://{user_defined_gcp_bucket}/assemblies/ NA","title":"Nextstrain Workflow"},{"location":"nextstrain_workflow/#nextstrain-workflow","text":"We use the publicly available Nexstrain workflow to generate Nextstrain builds, and then transfer the results using this transfer workflow. This workflow transfers the output file generated from the publicly available sarscov2_nextstrain workflow to a user specified google bucket. Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. The Google Bucket path describes where in the user google bucket the output file is transferred to.","title":"Nextstrain Workflow"},{"location":"nextstrain_workflow/#inputs","text":"workflow variable attribute (input syntax into workflow) google bucket path auspice_input_json this.auspice_input_json gs://{user_defined_gcp_bucket}/auspice_input_json/ combined_assemblies this.combined_assemblies gs://{user_defined_gcp_bucket}/combined_assemblies/ keep_list this.keep_list gs://{user_defined_gcp_bucket}/keep_list/ metadata_merged this.metadata gs://{user_defined_gcp_bucket}/metadata_merged/ ml_tree this.ml_tree gs://{user_defined_gcp_bucket}/ml_tree/ multiple_alignment this.multiple_alignment gs://{user_defined_gcp_bucket}/multiple_alignment/ node_data_jsons this.node_data_jsons gs://{user_defined_gcp_bucket}/node_data_jsons/ root_sequence_json this.root_sequence_json gs://{user_defined_gcp_bucket}/root_sequence_json/ subsampled_sequences this.subsampled_sequences gs://{user_defined_gcp_bucket}/subsampled_sequences/ time_tree this.time_tree gs://{user_defined_gcp_bucket}/time_tree/ tip_frequencies_json this.tip_frequencies_json gs://{user_defined_gcp_bucket}/tip_frequencies_json/ unmasked_snps this.unmasked_snps gs://{user_defined_gcp_bucket}/unmasked_snps/ out_dir gs://{user_defined_gcp_bucket}/assemblies/ NA","title":"Inputs"},{"location":"python_scripts/","text":"SARS-CoV-2 Python Scripts This repo contains three custom python scripts called in our SC2 wdl workflows. These scripts should be stored in a google bucket and linked to your workspace data within your Terra.bio workspace, so they can be used as inputs within each respective workflow. Below lists the three scripts and their associated workflows and further below provides greater detail about each script. calc_percent_coverage.py is called in the SC2_ont_assembly.wdl , SC2_illumina_pe_assembly.wdl , and SC2_illumina_se_assembly.wdl . nextclade_json_parser.py is called in the SC2_lineage_calling_and_results.wdl . concat_seq_metrics_and_lineage_results.py is called in the Sc2_lineage_calling_and_results.wdl . Table of Contents calc_percent_coverage-py nextclade_json_parser.py concat-seq-metrics-and-lineage-results-py details about working with sample sets calc-percent-coverage-py Overview This script is called in the SC2_illumina_pe_assembly.wdl , SC2_ilumina_se_assembly.wdl , and SC2_nanopore_assembly.wdl WDL workflows. These workflows act on individual samples (as opposed to sample sets), therefore this script also works on individual samples. The script reads in a consensus genome as a fasta file and calculates the percent coverage of the sample consensus genome. Specifically, the percent coverage is calculated as: percent_coverage = frac{number_non_ambigous_bases}{29903}, * 100 where the number_non_ambigous_bases is the number of basepair calls not including Ns in the sample consensus sequence and 29903 is the number of basepairs in the reference genome (NC_045512). Inputs flag description --sample_name sample name as recorded in the entity:sample_ID column in the terra data table --fasta_file renamed consensus sequence fasta file; generated during the ivar_consnesus (illumina data) or mekdata (ont data) and rename fasta tasks of the assembly workflows Outputs The script also records the number of aligned bases, the number of ambiguous bases (Ns), and the number of nonambiguous bases (A,G,C,T). The output is a csv file called {sample_name}_consensus_cvg_stats.csv and has the following column headers: column header description sample_name sample name as recorded in the entity:sample_ID column in the terra data table number_aligned_bases the total length of the consensus genome (including Ns) number_N_bases the number of ambiguous (N) bases in the consensus genome number_non_ambiguous_bases the number of non-ambiguous (A, C, G, T) bases in the consensus genome percent_coverage percent coverage of the reference genome represented in the consensus sequence as calculated above There is an example output in the example data directory within this repo. nextclade-json-parser-py Overview This script is called in the SC2_lineage_calling_and_results WDL workflow. This workflow acts on sample sets and so therefore this script also works on a sample set. This script is called in the parse_nextclade task within the workflow which can be seen in the SC2_lineage_calling_and_results.wdl workflow diagram in the README.md one directory out. Briefly, the workflow concatentates all consesnus sequences of the samples in the sample set into a single fasta file ( concatenate task). The concatentated fasta file is run through nextclade which generates a nextclade.json file ( nextclade task). Within the nextclade.json file is data for each sample consensus sequence inlcuding the nextclade clade designation, AA substitutions, deletions, insertions, etc. Generally, this script reads in the nextclade.json file, parses the json file to extract the data of interest, formats the data into a table and saves it as a csv file. Inputs flag description --nextclade_json the nextclade.json file generated in the nextclade task of the workflow. --project_name the project name of the sequencing run Outputs There are two outputs from this script, each accomplished from a seperate function within the script. Example outputs can be found in the example data directory within this repo. These functions are: extract_variant_list() function : This function generates a summmary of the AA substitions, insertions, and deletions for each sample within the nextclade.json file. The output is a csv file called {project_name}_nextclade_variant_summary.csv which is one of the files that is transfered to the google bucket as outputs of the workflow. The data is formatted such that each row corresponds to a either an AA substition, insertion, or deletion, such that each consensus seuqence can have more than one row of data. The csv file has the following column headers: column header description sample_name the sample name as listed in the fasta header (therefore there will be a \"CO-CDPHE-\" prefix added to the sample_name as listed in the entity column in the terra datatable) variant_name the full name of the variant formatted as {gene}_{refAA}{AApos}{altAA} (e.g. S_L452Q or S_L24del); For insertions the gene is not listed, the refAA is defined as \"ins\", the AA position is the nucleotide position in the genome and the altAA is listed as the string of nucleotides. So the variant is formated to look something like this: \"_ins1027T\" which would be interpreted as an insertion of a T nucleotide occured at genome position 1027 gene the gene where the AA substition, deletion or insertion occurs (e.g. N, S, ORF1a, M etc.). No gene is listed for insertions codon_position the codon position (or protien position) within the gene where the AA substition, deletion or insertion occured. For insertions it is the nucleotide genome position refAA the reference AA at the position where the AA substition, deletion, or insertion occured. For insertions the refAA is listed as \"ins\". altAA he AA in the consensus sequence at the position where the AA substition, deletion, or insertion occured. For insertions the altAA is the string of nucleotide base pairs that were inserted start_nuc_pos the starting nucleotide position within the genome where the AA substition, deletion, or insertion occured end_nuc_pos the ending nucleotide position within the genome where the AA substition, deletion, or insertion occured (for a single AA substition the start_ncu_pos and end_nuc_pos will be a difference of 3) get_nextclade() function: This function generates a summary of the nextclade designation, total nucleotide and AA substitions, total nucleotide and AA deletions, and total nucleotide insertions. The output file is called {seq_run}_nextclade_results.csv and is used as input for the concat_seq_metrics_and_lineage_results.py called in the results_table task in the workflow. The output file has the following column headers: column header description sample_name the sample name as listed in the fasta header (therefore there will be a \"CO-CDPHE-\" prefix added to the sample_name as listed in the entity column in the terra datatable) nextclade nextclade clade designation (e.g. 22C (Omicron)) total_nucleotide_mutations number of SNPs in consensus sequence total_nucleotide_deletions number of deletions in the consensus sequence total_nucleotide_insertions number of insertions in the consensus sequence total_AA_substitutions number of AA substitions in the consensus sequence total_AA_deletions number of AA deletions in the consensus sequence concat-seq-metrics-and-lineage-results-py Overview This script is called in the SC2_lineage_calling_and_results WDL workflow. This workflow acts on sample sets and so therefore this script also works on a sample set. This script is called in the results_table task within the workflow which can be seen in the SC2_lineage_calling_and_results.wdl workflow diagram in the README.md one directory out. Generally, this script pulls together a bunch of metadata and data regarding the consensus sequence and outputs the data in csv file. Inputs The script takes the following inputs: flag description --sample_name the sample_name variables for the set of samples as an array and written to a txt file using the wdl function write_lines --workbook_path the gcp file path to the workbook. The workbook can inlcude any column you'd like but must include at minimum the following columns: hsn , sample_id , --project_name , plate_name , run_name . These columns can be left blank if needed. --cov_out_filles the list of the cov_out variable (column in the terra data table) for the worfklow written to a text file. This variable is a file path to a file with the bam stats generated in the SC2_ont_assembly.wdl , SC2_ilumina_se_assembly.wdl or SC2_illumina_pe_assembly.wdl from the bam stats task. --percent_cvg_files the list of the percent_cvg_csv variable (column in the terra data table) for the worfklow written to a text file. This variable is a file path to a file with the bam stats generated in the SC2_ont_assembly.wdl , SC2_ilumina_se_assembly.wdl or SC2_illumina_pe_assembly.wdl workflows from the calc_percent_coverage.py script called during the calc_percent_cvg task. --assembler_version the assembler_version variable (column in the terra data table) for the worfklow . This is written to the terra data table during the SC2_ont_assembly.wdl , SC2_ilumina_se_assembly.wdl or SC2_illumina_pe_assembly.wdl workflows. --pangolin_lineage_csv this is the lineage report csv file generated from pangolin during the pangolin task --nextclade_clades_csv this is the {seq_run}_nextclade_results.csv file generated from the nextclade_json_parser.py script during the parse_nextclade task. --nextclade_variants_csv this is the {seq_run}_nextclade_variant_summary.csv file generated from the nextclade_json_parser.py script during the parse_nextclade task. --nextclade_version this is the nextclade version which is defined as output during the nextclade task. --project_name project_name from column in terra data table Outputs There are three outputs from this script. Example outputs can be found in the example data directory within this repo. {project_name}_sequencing_results.csv : summary of sequencing metrics for all samples within the sample set. Below is a table of the column headers and their description. Currently all headers from the sequencing workbook which get carried over to the terra datatable are inlcuded in this output file. The list below includes some but not all of the columns in the file. column header name description sample_name sample name hsn hsn (horizon serial number) primer_set name of primer set used for tiled amplicon squenicng (Artic V3, Artic V4, Artic V4.1, Midnight or COVIDSeqV3) percent_coverage percent coverage; the total proportion of the genome that is covered not including regions where an N is called for a basecall nextclade the nextclade clade assignment panoglin_lineage the pangolin lineage assignment pangolin_expanded_lineage the expanded panglin lineage assembler_version assembler software version (either bwa or minimpa depending on assembly workflow used) spike_mutations list of spike muations in the spike gene squence that correspond to key spike mutations identified in the sample consensus sequence (this column was created prior to VOCs and inlcudes spike mutatuations we were watching and has not been updated since) total_nucleotide_mutations number of SNPs in the consensus sequence genome total_AA substitutions number of amino acid substitions in the consensus sequence genome total_AA_deletions number of deletions in the consensus sequence genome mean_depth average number of reads per nucleotide site in the the conesnus sequnce genome number_aligned_bases total number of bases aligned to the refernece genome (including Ns; so pretty much tells you how much was cut of the ends of the genome) number_non_ambious_bases total number of non-N bases in the conesnus genome sequence number_seqs_in_fasta total number of sequences in the concensus fasta - should always be 1 total_nucleotide_deletions number of deletions in the consensus genome sequence total_nucleotide_insertions number of insertions in the consensus genome seqeunce num_reads total sequencing reads mean_base_quality mean quality score across all reads mean_map_quality mean mapping quality score for reads mapping to reference genome sequence number_N_bases number of bases called as N in the consensus genome sequence nextclade_version nextclade version panolgin_version pangolin version pangoLEARN_conflict from pangolin lineage report file pangolin_ambiguity_score from pangolin lineage report file pangolin_scorpio_call from pangolin lineage report file pangolin_scropio_support from pangolin lineage report file pangolin_scropio_conflict from pangolin lineage report file panoglin_scorpio_notes from pangolin lineage report file pangolin_designation_Version from pangolin lineage report file pangolin_scorpio_version from pangolin lineage report file pangolin_constellation_version from pangolin lineage report file pngolin_is_designated from pangolin lineage report file pangolin_qc_status from pangolin lineage report file pangolin_qc_notes from pangolin lineage report file panoglin_note from pangolin lineage report file seq_run sequencing run name tech_platform seuqencing platform (e.g. Illumina MiSeq, Illumina NextSeq, Oxford Nanopore GridION) read_type single or paired end fasta_header name of the fasta header for gisaid submission (e.g. CO-CDPHE-{accession_id}) analysis_date date assembly workflow ran {seq_run}_wgs_horizon_report.csv : for internal use, parsing sequencing results into LIMS.Below is a table of the column headers and their description. column header name description accession_id sample name percent_coverage percent coverage pangolin_lineage pangolin lineage pangolin_version pangolin version report_to_epi this column is meaningless now but have to keep Run_Date date assembly workflow ran pangoLEARN_version this column is also not used but we have to keep it Details About Working with Sample Sets Here I describe a way to create a single summary data table output for sample sets using a wwdl workflow in terra. (It's a bit clunkly but seems to work). Essentially this method enables one to create python lists from the columns in the terra data table when workflows are run as a sample set. The easiest way to explain this is with an example. So for example, in the concat_seq_metrics_and_lineage_results.py , there is the input flag --plate_name_file_list . As input for this flag I use ${write_lines(plate_name)} . The plate_name corresponds to the plate_name column in the terra data table. Each element in column is a string. The write_lines() wdl function will write each element it's own line to a text file. Thus, the input into the python script is really a text file with a list of plate names. I then wrote some code that reads in the text file and generates a python list, with each line being a new element in the list. So it looks something like this: plate_name_list = [] with open(plate_name_file_list) as f: for line in f: plate_name_list.append(line.strip()) Similiarly in some cases, instead of string variables being stored in a column wihtin the terra data table, file paths are stored (ie. the data type is a File or Array[File] in the case of sample sets). For example, in the concat_seq_metrics_and_lineage_results.py , there is the input flag --percent_cvg_file_list . As input for this flag I use ${write_lines(percent_cvg_csv_non_empty)} , where the percent_cvg_csv_non_empty variable corresponds to the column percent_cvg_csv. (note the non-empty part just means that the variable may be empty for some samples in the terra data table. To set the variable as input at the begining of the wdl I use: Array[File?] percent_cvg_csv ). Similiarly as above, the script will create a list of file paths from the text file. The script can then loop through the list of file paths, open each file, extract the data from that file, and store it in a list or other dataframe to be written out. Changelog updated 2023-03-09 to sync with universal naming switch over","title":"Python Scripts"},{"location":"python_scripts/#sars-cov-2-python-scripts","text":"This repo contains three custom python scripts called in our SC2 wdl workflows. These scripts should be stored in a google bucket and linked to your workspace data within your Terra.bio workspace, so they can be used as inputs within each respective workflow. Below lists the three scripts and their associated workflows and further below provides greater detail about each script. calc_percent_coverage.py is called in the SC2_ont_assembly.wdl , SC2_illumina_pe_assembly.wdl , and SC2_illumina_se_assembly.wdl . nextclade_json_parser.py is called in the SC2_lineage_calling_and_results.wdl . concat_seq_metrics_and_lineage_results.py is called in the Sc2_lineage_calling_and_results.wdl .","title":"SARS-CoV-2 Python Scripts"},{"location":"python_scripts/#table-of-contents","text":"calc_percent_coverage-py nextclade_json_parser.py concat-seq-metrics-and-lineage-results-py details about working with sample sets","title":"Table of Contents"},{"location":"python_scripts/#calc-percent-coverage-py","text":"","title":"calc-percent-coverage-py"},{"location":"python_scripts/#overview","text":"This script is called in the SC2_illumina_pe_assembly.wdl , SC2_ilumina_se_assembly.wdl , and SC2_nanopore_assembly.wdl WDL workflows. These workflows act on individual samples (as opposed to sample sets), therefore this script also works on individual samples. The script reads in a consensus genome as a fasta file and calculates the percent coverage of the sample consensus genome. Specifically, the percent coverage is calculated as: percent_coverage = frac{number_non_ambigous_bases}{29903}, * 100 where the number_non_ambigous_bases is the number of basepair calls not including Ns in the sample consensus sequence and 29903 is the number of basepairs in the reference genome (NC_045512).","title":"Overview"},{"location":"python_scripts/#inputs","text":"flag description --sample_name sample name as recorded in the entity:sample_ID column in the terra data table --fasta_file renamed consensus sequence fasta file; generated during the ivar_consnesus (illumina data) or mekdata (ont data) and rename fasta tasks of the assembly workflows","title":"Inputs"},{"location":"python_scripts/#outputs","text":"The script also records the number of aligned bases, the number of ambiguous bases (Ns), and the number of nonambiguous bases (A,G,C,T). The output is a csv file called {sample_name}_consensus_cvg_stats.csv and has the following column headers: column header description sample_name sample name as recorded in the entity:sample_ID column in the terra data table number_aligned_bases the total length of the consensus genome (including Ns) number_N_bases the number of ambiguous (N) bases in the consensus genome number_non_ambiguous_bases the number of non-ambiguous (A, C, G, T) bases in the consensus genome percent_coverage percent coverage of the reference genome represented in the consensus sequence as calculated above There is an example output in the example data directory within this repo.","title":"Outputs"},{"location":"python_scripts/#nextclade-json-parser-py","text":"","title":"nextclade-json-parser-py"},{"location":"python_scripts/#overview_1","text":"This script is called in the SC2_lineage_calling_and_results WDL workflow. This workflow acts on sample sets and so therefore this script also works on a sample set. This script is called in the parse_nextclade task within the workflow which can be seen in the SC2_lineage_calling_and_results.wdl workflow diagram in the README.md one directory out. Briefly, the workflow concatentates all consesnus sequences of the samples in the sample set into a single fasta file ( concatenate task). The concatentated fasta file is run through nextclade which generates a nextclade.json file ( nextclade task). Within the nextclade.json file is data for each sample consensus sequence inlcuding the nextclade clade designation, AA substitutions, deletions, insertions, etc. Generally, this script reads in the nextclade.json file, parses the json file to extract the data of interest, formats the data into a table and saves it as a csv file.","title":"Overview"},{"location":"python_scripts/#inputs_1","text":"flag description --nextclade_json the nextclade.json file generated in the nextclade task of the workflow. --project_name the project name of the sequencing run","title":"Inputs"},{"location":"python_scripts/#outputs_1","text":"There are two outputs from this script, each accomplished from a seperate function within the script. Example outputs can be found in the example data directory within this repo. These functions are: extract_variant_list() function : This function generates a summmary of the AA substitions, insertions, and deletions for each sample within the nextclade.json file. The output is a csv file called {project_name}_nextclade_variant_summary.csv which is one of the files that is transfered to the google bucket as outputs of the workflow. The data is formatted such that each row corresponds to a either an AA substition, insertion, or deletion, such that each consensus seuqence can have more than one row of data. The csv file has the following column headers: column header description sample_name the sample name as listed in the fasta header (therefore there will be a \"CO-CDPHE-\" prefix added to the sample_name as listed in the entity column in the terra datatable) variant_name the full name of the variant formatted as {gene}_{refAA}{AApos}{altAA} (e.g. S_L452Q or S_L24del); For insertions the gene is not listed, the refAA is defined as \"ins\", the AA position is the nucleotide position in the genome and the altAA is listed as the string of nucleotides. So the variant is formated to look something like this: \"_ins1027T\" which would be interpreted as an insertion of a T nucleotide occured at genome position 1027 gene the gene where the AA substition, deletion or insertion occurs (e.g. N, S, ORF1a, M etc.). No gene is listed for insertions codon_position the codon position (or protien position) within the gene where the AA substition, deletion or insertion occured. For insertions it is the nucleotide genome position refAA the reference AA at the position where the AA substition, deletion, or insertion occured. For insertions the refAA is listed as \"ins\". altAA he AA in the consensus sequence at the position where the AA substition, deletion, or insertion occured. For insertions the altAA is the string of nucleotide base pairs that were inserted start_nuc_pos the starting nucleotide position within the genome where the AA substition, deletion, or insertion occured end_nuc_pos the ending nucleotide position within the genome where the AA substition, deletion, or insertion occured (for a single AA substition the start_ncu_pos and end_nuc_pos will be a difference of 3) get_nextclade() function: This function generates a summary of the nextclade designation, total nucleotide and AA substitions, total nucleotide and AA deletions, and total nucleotide insertions. The output file is called {seq_run}_nextclade_results.csv and is used as input for the concat_seq_metrics_and_lineage_results.py called in the results_table task in the workflow. The output file has the following column headers: column header description sample_name the sample name as listed in the fasta header (therefore there will be a \"CO-CDPHE-\" prefix added to the sample_name as listed in the entity column in the terra datatable) nextclade nextclade clade designation (e.g. 22C (Omicron)) total_nucleotide_mutations number of SNPs in consensus sequence total_nucleotide_deletions number of deletions in the consensus sequence total_nucleotide_insertions number of insertions in the consensus sequence total_AA_substitutions number of AA substitions in the consensus sequence total_AA_deletions number of AA deletions in the consensus sequence","title":"Outputs"},{"location":"python_scripts/#concat-seq-metrics-and-lineage-results-py","text":"","title":"concat-seq-metrics-and-lineage-results-py"},{"location":"python_scripts/#overview_2","text":"This script is called in the SC2_lineage_calling_and_results WDL workflow. This workflow acts on sample sets and so therefore this script also works on a sample set. This script is called in the results_table task within the workflow which can be seen in the SC2_lineage_calling_and_results.wdl workflow diagram in the README.md one directory out. Generally, this script pulls together a bunch of metadata and data regarding the consensus sequence and outputs the data in csv file.","title":"Overview"},{"location":"python_scripts/#inputs_2","text":"The script takes the following inputs: flag description --sample_name the sample_name variables for the set of samples as an array and written to a txt file using the wdl function write_lines --workbook_path the gcp file path to the workbook. The workbook can inlcude any column you'd like but must include at minimum the following columns: hsn , sample_id , --project_name , plate_name , run_name . These columns can be left blank if needed. --cov_out_filles the list of the cov_out variable (column in the terra data table) for the worfklow written to a text file. This variable is a file path to a file with the bam stats generated in the SC2_ont_assembly.wdl , SC2_ilumina_se_assembly.wdl or SC2_illumina_pe_assembly.wdl from the bam stats task. --percent_cvg_files the list of the percent_cvg_csv variable (column in the terra data table) for the worfklow written to a text file. This variable is a file path to a file with the bam stats generated in the SC2_ont_assembly.wdl , SC2_ilumina_se_assembly.wdl or SC2_illumina_pe_assembly.wdl workflows from the calc_percent_coverage.py script called during the calc_percent_cvg task. --assembler_version the assembler_version variable (column in the terra data table) for the worfklow . This is written to the terra data table during the SC2_ont_assembly.wdl , SC2_ilumina_se_assembly.wdl or SC2_illumina_pe_assembly.wdl workflows. --pangolin_lineage_csv this is the lineage report csv file generated from pangolin during the pangolin task --nextclade_clades_csv this is the {seq_run}_nextclade_results.csv file generated from the nextclade_json_parser.py script during the parse_nextclade task. --nextclade_variants_csv this is the {seq_run}_nextclade_variant_summary.csv file generated from the nextclade_json_parser.py script during the parse_nextclade task. --nextclade_version this is the nextclade version which is defined as output during the nextclade task. --project_name project_name from column in terra data table","title":"Inputs"},{"location":"python_scripts/#outputs_2","text":"There are three outputs from this script. Example outputs can be found in the example data directory within this repo. {project_name}_sequencing_results.csv : summary of sequencing metrics for all samples within the sample set. Below is a table of the column headers and their description. Currently all headers from the sequencing workbook which get carried over to the terra datatable are inlcuded in this output file. The list below includes some but not all of the columns in the file. column header name description sample_name sample name hsn hsn (horizon serial number) primer_set name of primer set used for tiled amplicon squenicng (Artic V3, Artic V4, Artic V4.1, Midnight or COVIDSeqV3) percent_coverage percent coverage; the total proportion of the genome that is covered not including regions where an N is called for a basecall nextclade the nextclade clade assignment panoglin_lineage the pangolin lineage assignment pangolin_expanded_lineage the expanded panglin lineage assembler_version assembler software version (either bwa or minimpa depending on assembly workflow used) spike_mutations list of spike muations in the spike gene squence that correspond to key spike mutations identified in the sample consensus sequence (this column was created prior to VOCs and inlcudes spike mutatuations we were watching and has not been updated since) total_nucleotide_mutations number of SNPs in the consensus sequence genome total_AA substitutions number of amino acid substitions in the consensus sequence genome total_AA_deletions number of deletions in the consensus sequence genome mean_depth average number of reads per nucleotide site in the the conesnus sequnce genome number_aligned_bases total number of bases aligned to the refernece genome (including Ns; so pretty much tells you how much was cut of the ends of the genome) number_non_ambious_bases total number of non-N bases in the conesnus genome sequence number_seqs_in_fasta total number of sequences in the concensus fasta - should always be 1 total_nucleotide_deletions number of deletions in the consensus genome sequence total_nucleotide_insertions number of insertions in the consensus genome seqeunce num_reads total sequencing reads mean_base_quality mean quality score across all reads mean_map_quality mean mapping quality score for reads mapping to reference genome sequence number_N_bases number of bases called as N in the consensus genome sequence nextclade_version nextclade version panolgin_version pangolin version pangoLEARN_conflict from pangolin lineage report file pangolin_ambiguity_score from pangolin lineage report file pangolin_scorpio_call from pangolin lineage report file pangolin_scropio_support from pangolin lineage report file pangolin_scropio_conflict from pangolin lineage report file panoglin_scorpio_notes from pangolin lineage report file pangolin_designation_Version from pangolin lineage report file pangolin_scorpio_version from pangolin lineage report file pangolin_constellation_version from pangolin lineage report file pngolin_is_designated from pangolin lineage report file pangolin_qc_status from pangolin lineage report file pangolin_qc_notes from pangolin lineage report file panoglin_note from pangolin lineage report file seq_run sequencing run name tech_platform seuqencing platform (e.g. Illumina MiSeq, Illumina NextSeq, Oxford Nanopore GridION) read_type single or paired end fasta_header name of the fasta header for gisaid submission (e.g. CO-CDPHE-{accession_id}) analysis_date date assembly workflow ran {seq_run}_wgs_horizon_report.csv : for internal use, parsing sequencing results into LIMS.Below is a table of the column headers and their description. column header name description accession_id sample name percent_coverage percent coverage pangolin_lineage pangolin lineage pangolin_version pangolin version report_to_epi this column is meaningless now but have to keep Run_Date date assembly workflow ran pangoLEARN_version this column is also not used but we have to keep it","title":"Outputs"},{"location":"python_scripts/#details-about-working-with-sample-sets","text":"Here I describe a way to create a single summary data table output for sample sets using a wwdl workflow in terra. (It's a bit clunkly but seems to work). Essentially this method enables one to create python lists from the columns in the terra data table when workflows are run as a sample set. The easiest way to explain this is with an example. So for example, in the concat_seq_metrics_and_lineage_results.py , there is the input flag --plate_name_file_list . As input for this flag I use ${write_lines(plate_name)} . The plate_name corresponds to the plate_name column in the terra data table. Each element in column is a string. The write_lines() wdl function will write each element it's own line to a text file. Thus, the input into the python script is really a text file with a list of plate names. I then wrote some code that reads in the text file and generates a python list, with each line being a new element in the list. So it looks something like this: plate_name_list = [] with open(plate_name_file_list) as f: for line in f: plate_name_list.append(line.strip()) Similiarly in some cases, instead of string variables being stored in a column wihtin the terra data table, file paths are stored (ie. the data type is a File or Array[File] in the case of sample sets). For example, in the concat_seq_metrics_and_lineage_results.py , there is the input flag --percent_cvg_file_list . As input for this flag I use ${write_lines(percent_cvg_csv_non_empty)} , where the percent_cvg_csv_non_empty variable corresponds to the column percent_cvg_csv. (note the non-empty part just means that the variable may be empty for some samples in the terra data table. To set the variable as input at the begining of the wdl I use: Array[File?] percent_cvg_csv ). Similiarly as above, the script will create a list of file paths from the text file. The script can then loop through the list of file paths, open each file, extract the data from that file, and store it in a list or other dataframe to be written out.","title":"Details About Working with Sample Sets"},{"location":"python_scripts/#changelog","text":"updated 2023-03-09 to sync with universal naming switch over","title":"Changelog"},{"location":"results_workflow/","text":"Results Workflows The following workflows will perform clade and lineage assignment using Nextclade and Pangolin, respectively. The SC2_lineage_calling_and_results.wdl should be run following assembly with one of the three above reference-based assembly workflows. The SC2_mulitfasta_lineage_calling can be run on any multifasta file and is not dependent on any of the above workflows. Lineage Calling and Results Workflow File: SC2_lineage_calling_and_results.wdl This workflow should be run following assembly with one of the three reference based assembly workflows. The workflow accepts \"sample_set\" as the root entity type and uses the data table from any of the three assembly workflows. All three assembly workflows (illumina pe, illumina se, and ont) are compatible with this workflow. Breifly the workflow performs the following: concatenates consensus sequences from the sequencing run into a single fasta file runs panoglin on the concatenated fasta file runs nextclade on the concatenated fasta file parses the nextclade json file output using the nextclade_json_parser.py script which pulls out clade and nucleotide and animo acid changes information and converts it to a tabular format. Concaenates sequencing assembly metrics (e.g. percent coverage, assembler version), lineage and clade information, and sequence metadata (e.g. plate name, sample well location) into a single csv file. Generates a csv file with sequencing assembly metrics and lineage information that can be used to parse sequencing data into our LIMS. Transfers intermediate and summary files to a user defined google bucket. Inputs Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. These variables were either in the original terra datatable as inputs for the assembly workflow (see referece based assembly workflow inputs sections above for more details) or added as outputs during the assemlby workflow (see reference based assembly workflow outputs sections for more details). The \"workspace.\" syntax refers Terra to pull the variable from the terra workspace data. Workspace data is describe in the Getting Started drop down menu above. workflow variable attribute (input syntax into workflow) assembler_version_array this.sample{terra_datatable_name}s.assembler_version concat_seq_results_py workspace.covid_concat_results_py cov_out this.sample{terra_datatable_name}s.cov_out nextclade_json_parser_py workspace.covid_nextclade_json_parser_py out_dir_array this.sample{terra_datatable_name}s.out_dir percent_cvg_csv this.sample{terra_datatable_name}s.percent_cvg_csv project_name_array this.sample{terra_datatable_name}s.project_name renamed_consensus this.sample{terra_datatable_name}s.renamed_consesnus sample_name this.sample{terra_datatable_name}s.sample{terra_datatable_name}_id workbook_path_array this.sample{terra_datatable_name}s.workbook_path Outputs This workflow generates several output files which are transfered to the user defined user google bucket as defined by this.sample{terra_datatable_name}s.out_dir. The table below details each output. For more detailed regarding the values in each column for the outputs see either the software readmes or the readme for the specific python script as listed in the description. output variable name file_name description google bucket path cat_fastas concatenate_assemblies.fasta all consesnus sequences from assembly in a single fasta file gs://{user_defined_gcp_bucket}/multifasta/ nextclade_clades_csv {seq_run}_nextclade_results.csv csv file generated from the nextclade_json_parser.py script detailing the clade for each seqeunce gs://{user_defined_gcp_bucket}/nextclade_out/ nextclade_csv nextclade.csv csv file generated from nextclade gs://{user_defined_gcp_bucket}/nextclade_out/ nextclade_json nextclade.json json file generated from nextclade; this json file is parsed using the nextclade_json_parser.py script and key info is pulled out and converted into a tablular format in the nextclade_clades_csv , nextclade_variants_csv and sequencing_results.csv files (see the readme for the nextclade_json_parser.py script for more details) gs://{user_defined_gcp_bucket}/nextclade_out/ nextclade_variants_csv {seq_run}_nextclade_variant_summary.csv csv file generated from the nextclade_json_parser.py script detailing the nucleotide and amino acid changes for each seqeunce gs://{user_defined_gcp_bucket}/summary_results/ nextclade_version N/A version of nextclade N/A pangolin_lineage pangolin_lineage_report.csv lineage report generated from pangolin gs://{user_defined_gcp_bucket}/pangolin/ pangolin_version N/A version of panoglin N/A sequencing_results_csv {seq_run}_sequencing_results.csv summary of the sequencing metrics and lineage/clade assignments for each sequence generated from the concat_seq_metrics_and_lineage_results.py script. see the concat_seq_metrics_and_lineage_results.py readme for more details. gs://{user_defined_gcp_bucket}/summary_results/ wgs_horizon_report_csv {seq_run}_wgs_horizon_report.csv results csv used for parsing results into our LIMS. This file is generated from the concat_seq_metrics_and_lineage_results.py script. see the concat_seq_metrics_and_lineage_results.py readme for more details. gs://{user_defined_gcp_bucket}/summary_results/ Multifasta Lineage Calling and Results Workflow File: SC2_multifasta_lineage_calling.wdl This workflow will perfrom lineage and clade assignment using a concatenated fasta file as input. This workflow is a stand alone workflow and does not depend on any of the previous reference-based assembly workflows. The workflow accepts \"sample\" as the root entity type and uses a simple two column data table (see inputs below). Brienfly this workflow will perform the following: Run nextclade Run panoglin transfer outputs to a user defined google bucket. Inputs Terra datatable. You will need to create a terra data that contains the following two columns: entity:samle_id : should contain the prefix that you would like for you output files multifasta : google bucket path to the multi-sequence fasta file Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.\" syntax refers Terra to pull the variable from the terra datatable (#1 above). workflow variable attribute (input syntax into workflow) multifasta this.multifasta sample_id this.sample{terra_datatable_name}_id out_dir \"gs://{path to user defined google bucket}\" Outputs This workflow generates several output files which are transfered to the user defined user google bucket as defined by the out_dir variable. The table below details each output. output variable name file_name description google bucket path nextclade_version N/A nextclade version N/A nextclade_json {sample_id}_nextclade_json json file generated from nextclade gs://{user_defined_gcp_bucket}/nextclade_out auspice_json {sample_id}_nextclade.auspice.json auspice json file generated from nextclade gs://{user_defined_gcp_bucket}/nextclade_out nextclade_csv {sample_id}_nextclade.csv csv file generated from nextclade gs://{user_defined_gcp_bucket}/nextclade_out pangolin_version N/A pangolin version N/A pangolin_lineage {sample_id}_panoglin_lineage_report.csv lineage report generated from pangolin gs://{user_defined_gcp_bucket}/pangolin_out","title":"Results Workflows"},{"location":"results_workflow/#results-workflows","text":"The following workflows will perform clade and lineage assignment using Nextclade and Pangolin, respectively. The SC2_lineage_calling_and_results.wdl should be run following assembly with one of the three above reference-based assembly workflows. The SC2_mulitfasta_lineage_calling can be run on any multifasta file and is not dependent on any of the above workflows.","title":"Results Workflows"},{"location":"results_workflow/#lineage-calling-and-results-workflow","text":"File: SC2_lineage_calling_and_results.wdl This workflow should be run following assembly with one of the three reference based assembly workflows. The workflow accepts \"sample_set\" as the root entity type and uses the data table from any of the three assembly workflows. All three assembly workflows (illumina pe, illumina se, and ont) are compatible with this workflow. Breifly the workflow performs the following: concatenates consensus sequences from the sequencing run into a single fasta file runs panoglin on the concatenated fasta file runs nextclade on the concatenated fasta file parses the nextclade json file output using the nextclade_json_parser.py script which pulls out clade and nucleotide and animo acid changes information and converts it to a tabular format. Concaenates sequencing assembly metrics (e.g. percent coverage, assembler version), lineage and clade information, and sequence metadata (e.g. plate name, sample well location) into a single csv file. Generates a csv file with sequencing assembly metrics and lineage information that can be used to parse sequencing data into our LIMS. Transfers intermediate and summary files to a user defined google bucket.","title":"Lineage Calling and Results Workflow"},{"location":"results_workflow/#inputs","text":"Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. These variables were either in the original terra datatable as inputs for the assembly workflow (see referece based assembly workflow inputs sections above for more details) or added as outputs during the assemlby workflow (see reference based assembly workflow outputs sections for more details). The \"workspace.\" syntax refers Terra to pull the variable from the terra workspace data. Workspace data is describe in the Getting Started drop down menu above. workflow variable attribute (input syntax into workflow) assembler_version_array this.sample{terra_datatable_name}s.assembler_version concat_seq_results_py workspace.covid_concat_results_py cov_out this.sample{terra_datatable_name}s.cov_out nextclade_json_parser_py workspace.covid_nextclade_json_parser_py out_dir_array this.sample{terra_datatable_name}s.out_dir percent_cvg_csv this.sample{terra_datatable_name}s.percent_cvg_csv project_name_array this.sample{terra_datatable_name}s.project_name renamed_consensus this.sample{terra_datatable_name}s.renamed_consesnus sample_name this.sample{terra_datatable_name}s.sample{terra_datatable_name}_id workbook_path_array this.sample{terra_datatable_name}s.workbook_path","title":"Inputs"},{"location":"results_workflow/#outputs","text":"This workflow generates several output files which are transfered to the user defined user google bucket as defined by this.sample{terra_datatable_name}s.out_dir. The table below details each output. For more detailed regarding the values in each column for the outputs see either the software readmes or the readme for the specific python script as listed in the description. output variable name file_name description google bucket path cat_fastas concatenate_assemblies.fasta all consesnus sequences from assembly in a single fasta file gs://{user_defined_gcp_bucket}/multifasta/ nextclade_clades_csv {seq_run}_nextclade_results.csv csv file generated from the nextclade_json_parser.py script detailing the clade for each seqeunce gs://{user_defined_gcp_bucket}/nextclade_out/ nextclade_csv nextclade.csv csv file generated from nextclade gs://{user_defined_gcp_bucket}/nextclade_out/ nextclade_json nextclade.json json file generated from nextclade; this json file is parsed using the nextclade_json_parser.py script and key info is pulled out and converted into a tablular format in the nextclade_clades_csv , nextclade_variants_csv and sequencing_results.csv files (see the readme for the nextclade_json_parser.py script for more details) gs://{user_defined_gcp_bucket}/nextclade_out/ nextclade_variants_csv {seq_run}_nextclade_variant_summary.csv csv file generated from the nextclade_json_parser.py script detailing the nucleotide and amino acid changes for each seqeunce gs://{user_defined_gcp_bucket}/summary_results/ nextclade_version N/A version of nextclade N/A pangolin_lineage pangolin_lineage_report.csv lineage report generated from pangolin gs://{user_defined_gcp_bucket}/pangolin/ pangolin_version N/A version of panoglin N/A sequencing_results_csv {seq_run}_sequencing_results.csv summary of the sequencing metrics and lineage/clade assignments for each sequence generated from the concat_seq_metrics_and_lineage_results.py script. see the concat_seq_metrics_and_lineage_results.py readme for more details. gs://{user_defined_gcp_bucket}/summary_results/ wgs_horizon_report_csv {seq_run}_wgs_horizon_report.csv results csv used for parsing results into our LIMS. This file is generated from the concat_seq_metrics_and_lineage_results.py script. see the concat_seq_metrics_and_lineage_results.py readme for more details. gs://{user_defined_gcp_bucket}/summary_results/","title":"Outputs"},{"location":"results_workflow/#multifasta-lineage-calling-and-results-workflow","text":"File: SC2_multifasta_lineage_calling.wdl This workflow will perfrom lineage and clade assignment using a concatenated fasta file as input. This workflow is a stand alone workflow and does not depend on any of the previous reference-based assembly workflows. The workflow accepts \"sample\" as the root entity type and uses a simple two column data table (see inputs below). Brienfly this workflow will perform the following: Run nextclade Run panoglin transfer outputs to a user defined google bucket.","title":"Multifasta Lineage Calling and Results Workflow"},{"location":"results_workflow/#inputs_1","text":"Terra datatable. You will need to create a terra data that contains the following two columns: entity:samle_id : should contain the prefix that you would like for you output files multifasta : google bucket path to the multi-sequence fasta file Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.\" syntax refers Terra to pull the variable from the terra datatable (#1 above). workflow variable attribute (input syntax into workflow) multifasta this.multifasta sample_id this.sample{terra_datatable_name}_id out_dir \"gs://{path to user defined google bucket}\"","title":"Inputs"},{"location":"results_workflow/#outputs_1","text":"This workflow generates several output files which are transfered to the user defined user google bucket as defined by the out_dir variable. The table below details each output. output variable name file_name description google bucket path nextclade_version N/A nextclade version N/A nextclade_json {sample_id}_nextclade_json json file generated from nextclade gs://{user_defined_gcp_bucket}/nextclade_out auspice_json {sample_id}_nextclade.auspice.json auspice json file generated from nextclade gs://{user_defined_gcp_bucket}/nextclade_out nextclade_csv {sample_id}_nextclade.csv csv file generated from nextclade gs://{user_defined_gcp_bucket}/nextclade_out pangolin_version N/A pangolin version N/A pangolin_lineage {sample_id}_panoglin_lineage_report.csv lineage report generated from pangolin gs://{user_defined_gcp_bucket}/pangolin_out","title":"Outputs"},{"location":"setup/","text":"Setup Prior to running any of the workflows, you must set up the terra table and link reference files and custom python scripts to your workspace data. Below is a table detailing the workspace data you will need to set up. Workspace Data The reference files can be found in this repository in the workspace_data directory. Python scripts can be found in the listed repsoitory directory. workspace variable name workflow file name description adapters_and_contaminants_fa SC2_illumina_pe_assembly Adapters_plus_PhiX_174.fasta adapters sequences and contaminant sequences removed during fastq cleaning and filtering using SeqyClean. Thanks to Erin Young at Utah Public Health Laboratory for providing this file! covid_genome_gff SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly NC_045512-2_reference.gff whole genome reference sequence annotation file in gff format (we use NCBI genbank ID MN908947.3) covid_genome_fa SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly MN908947-2_reference.fasta SARS-CoV-2 whole genome reference sequence in fasta format (we use NCBI genbank ID MN908947.3) artic_v3_bed SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly artic_V3_nCoV-2019.primer.bed primer bed file for the Artic V3 tiled amplicon primer set. Thanks to Theiagen Genomics for providing this file! artic_v4_bed SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly artic_V4_nCoV-2019.primer.bed primer bed file for the Artic V4 tiled amplicon primer set. Thanks to Theiagen Genomics for providing this file! artic_v4-1_bed SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly artic_V4-1_nCoV-2019.primer.bed primer bed file for the Artic V4.1 tiled amplicon primer set. Thanks to Theiagen Genomics for providing this file! artic_v4-1_s_gene_amplicons SC2_illumina_pe_assembly , SC2_ont_assembly artic_v4_1_s_gene_amplicons.tsv artic_v4-1_s_gene_primer_bed SC2_illumina_pe_assembly , SC2_ont_assembly S_gene_V4-1_nCoV-2021.primer.bed midnight_bed SC2_ont_assembly Midnight_Primers_SARS-CoV-2.scheme.bed primer bed file for the Midnight tiled amplicon primer set. Thanks to Theiagen Genomics for providing this file! covid_voc_annotations_tsv SC2_wastewater_variant_calling workflow SC2_voc_annotations_20220711.tsv For wastewater only. List of amino acid (AA) substitutions and lineages containing those AA substitutions; for a lineage to be associated with a given AA substitution, 90% of publicly available sequences must contain the AA substitution (the 90% cutoff was determined using outbreak.info) covid_voc_bed_tsv SC2_wastewater_variant_calling workflow . SC2_voc_mutations_20220711.tsv For wastewater only. List of nucleotide genome positions in relation to the MN908947.3 reference genome of know mutations covid_calc_per_cov_py SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly calc_percent_coverage.py see detailed description in the readme file found in ./python_scripts/ repo directory covid_nextclade_json_parser_py SC2_lineage_calling_and_results nextclade_json_parser.py see detailed description in the readme file found in ./python_scripts/ repo directory covid_concat_results_py SC2_lineage_calling_and_results concat_seq_metrics_and_lineages_results.py see detailed description in the readme file found in ./python_scripts repo directory","title":"Setup"},{"location":"setup/#setup","text":"Prior to running any of the workflows, you must set up the terra table and link reference files and custom python scripts to your workspace data. Below is a table detailing the workspace data you will need to set up.","title":"Setup"},{"location":"setup/#workspace-data","text":"The reference files can be found in this repository in the workspace_data directory. Python scripts can be found in the listed repsoitory directory. workspace variable name workflow file name description adapters_and_contaminants_fa SC2_illumina_pe_assembly Adapters_plus_PhiX_174.fasta adapters sequences and contaminant sequences removed during fastq cleaning and filtering using SeqyClean. Thanks to Erin Young at Utah Public Health Laboratory for providing this file! covid_genome_gff SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly NC_045512-2_reference.gff whole genome reference sequence annotation file in gff format (we use NCBI genbank ID MN908947.3) covid_genome_fa SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly MN908947-2_reference.fasta SARS-CoV-2 whole genome reference sequence in fasta format (we use NCBI genbank ID MN908947.3) artic_v3_bed SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly artic_V3_nCoV-2019.primer.bed primer bed file for the Artic V3 tiled amplicon primer set. Thanks to Theiagen Genomics for providing this file! artic_v4_bed SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly artic_V4_nCoV-2019.primer.bed primer bed file for the Artic V4 tiled amplicon primer set. Thanks to Theiagen Genomics for providing this file! artic_v4-1_bed SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly artic_V4-1_nCoV-2019.primer.bed primer bed file for the Artic V4.1 tiled amplicon primer set. Thanks to Theiagen Genomics for providing this file! artic_v4-1_s_gene_amplicons SC2_illumina_pe_assembly , SC2_ont_assembly artic_v4_1_s_gene_amplicons.tsv artic_v4-1_s_gene_primer_bed SC2_illumina_pe_assembly , SC2_ont_assembly S_gene_V4-1_nCoV-2021.primer.bed midnight_bed SC2_ont_assembly Midnight_Primers_SARS-CoV-2.scheme.bed primer bed file for the Midnight tiled amplicon primer set. Thanks to Theiagen Genomics for providing this file! covid_voc_annotations_tsv SC2_wastewater_variant_calling workflow SC2_voc_annotations_20220711.tsv For wastewater only. List of amino acid (AA) substitutions and lineages containing those AA substitutions; for a lineage to be associated with a given AA substitution, 90% of publicly available sequences must contain the AA substitution (the 90% cutoff was determined using outbreak.info) covid_voc_bed_tsv SC2_wastewater_variant_calling workflow . SC2_voc_mutations_20220711.tsv For wastewater only. List of nucleotide genome positions in relation to the MN908947.3 reference genome of know mutations covid_calc_per_cov_py SC2_illumina_pe_assembly , SC2_illumina_se_assembly , SC2_ont_assembly calc_percent_coverage.py see detailed description in the readme file found in ./python_scripts/ repo directory covid_nextclade_json_parser_py SC2_lineage_calling_and_results nextclade_json_parser.py see detailed description in the readme file found in ./python_scripts/ repo directory covid_concat_results_py SC2_lineage_calling_and_results concat_seq_metrics_and_lineages_results.py see detailed description in the readme file found in ./python_scripts repo directory","title":"Workspace Data"},{"location":"transfer_workflow/","text":"Transfer Workflows After assembly, the assembly workflow outputs are then transferred from Terra back to a user specified google bucket for additional analysis and storage. Each workflow accepts \"sample\" as the root entity type and uses the data table from the corresponding assembly workflow data table that has been filled in with file paths to the outputs. Illumina PE File: SC2_transfer_illumina_pe_assembly.wdl This workflow transfers the output file generated from SC2_illumina_pe_assemby to a user specified google bucket. Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. The Google Bucket path describes where in the user google bucket the output file is transferred to. workflow variable attribute (input syntax into workflow) google bucket path consensus this.consensus gs://{user_defined_gcp_bucket}/assemblies/ covhist_out this.coverage_hist gs://{user_defined_gcp_bucket}/bam_stats/ cov_out this.coverage_out gs://{user_defined_gcp_bucket}/bam_stats/ fastqc_clean1_html this.fastq_clean1_html gs://{user_defined_gcp_bucket}/fastqc/ fastqc_clean1_zip this.fastqc_clean1_zip gs://{user_defined_gcp_bucket}/fastqc/ fastqc_clean2_html this.fastqc_clean2_html gs://{user_defined_gcp_bucket}/fastqc/ fastqc_clean2_zip this.fastqc_clean2_zip gs://{user_defined_gcp_bucket}/fastqc/ filtered_reads_1 this.filtered_reads_1 gs://{user_defined_gcp_bucket}/seqyclean/ filtered_reads_2 this.filtered_reads_2 gs://{user_defined_gcp_bucket}/seqclean/ flagstat_out this.flagstat_out gs://{user_defined_gcp_bucket}/bamstats/ out_dir this.out_dir N/A renamed_consensus this.renamed_consensus gs://{user_defined_gcp_bucket}/assemblies/ trimsort_bam this.trimsort_bam gs://{user_defined_gcp_bucket}/alignments/ trimsort_bamindex this.trimsort_bamindex gs://{user_defined_gcp_bucket}/alignments/ variants this.variants gs://{user_defined_gcp_bucket}/variants/ Illumina SE This workflow is not longer maintained. File: SC2_transfer_illumina_se_assembly.wdl This workflow transfers the output file generated from SC2_illumina_se_assemby to a user specified google bucket. Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. The Google Bucket path describes where in the user google bucket the output file is transferred to. workflow variable attribute (input syntax into workflow) google bucket path adapter_stats this.sample{terra_datatable_name}s.adapter_stats gs://{user_defined_gcp_bucket}/filter_reads/ consensus this.sample{terra_datatable_name}s.consensus gs://{user_defined_gcp_bucket}/assemblies/ covhist_out this.sample{terra_datatable_name}s.coverage_hist gs://{user_defined_gcp_bucket}/bam_stats/ cov_out this.sample{terra_datatable_name}s.coverage_out gs://{user_defined_gcp_bucket}/bam_stats/ fastqc_clean_html this.sample{terra_datatable_name}s.fastq_clean_html gs://{user_defined_gcp_bucket}/fastqc/ fastqc_clean_zip this.sample{terra_datatable_name}s.fastqc_clean_zip gs://{user_defined_gcp_bucket}/fastqc/ filtered_reads this.sample{terra_datatable_name}s.filtered_reads gs://{user_defined_gcp_bucket}/seqyclean/ flagstat_out this.sample{terra_datatable_name}s.flagstat_out gs://{user_defined_gcp_bucket}/bamstats/ out_dir this.sample{terra_datatable_name}s.out_dir N/A PhiX_stats this.sample{terra_datatable_name}s.PhiX_stats gs://{user_defined_gcp_bucket}/filtered_reads/ renamed_consensus this.sample{terra_datatable_name}s.renmaed_consensus gs://{user_defined_gcp_bucket}/assemblies/ trimsort_bam this.sample{terra_datatable_name}s.trimsort_bam gs://{user_defined_gcp_bucket}/alignments/ trimsort_bamindex this.sample{terra_datatable_name}s.trimsort_bamindex gs://{user_defined_gcp_bucket}/alignments/ variants this.sample{terra_datatable_name}s.variants gs://{user_defined_gcp_bucket}/variants/ Oxford Nanopore Technologies (ONT) File: SC2_transfer_ont_assembly.wdl This workflow transfers the output file generated from SC2_ont_assemby to a user specified google bucket. Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. The Google Bucket path describes where in the user google bucket the output file is transferred to. workflow variable attribute (input syntax into workflow) google bucket path when transfered covhist_out this.coverage_hist gs://{user_defined_gcp_bucket}/bam_stats/ cov_out this.coverage_out gs://{user_defined_gcp_bucket}/bam_stats/ filtered_fastq this.filtered_fastq gs://{user_defined_gcp_bucket}/filtered_fastq/ flagstat_out this.flagstat_out gs://{user_defined_gcp_bucket}/bamstats/ out_dir this.out_dir N/A renamed_consensus this.renmaed_consensus gs://{user_defined_gcp_bucket}/assemblies/ samstats_out this.samstats_out gs://{user_defined_gcp_bucket}/bam_stats/ scaffold_consensus this.scaffold_consensus gs://{user_defined_gcp_bucket}/assemblies/ trimsort_bam this.trimsort_bam gs://{user_defined_gcp_bucket}/alignments/ variants this.variants gs://{user_defined_gcp_bucket}/variants/","title":"Transfer Workflows"},{"location":"transfer_workflow/#transfer-workflows","text":"After assembly, the assembly workflow outputs are then transferred from Terra back to a user specified google bucket for additional analysis and storage. Each workflow accepts \"sample\" as the root entity type and uses the data table from the corresponding assembly workflow data table that has been filled in with file paths to the outputs.","title":"Transfer Workflows"},{"location":"transfer_workflow/#illumina-pe","text":"File: SC2_transfer_illumina_pe_assembly.wdl This workflow transfers the output file generated from SC2_illumina_pe_assemby to a user specified google bucket. Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. The Google Bucket path describes where in the user google bucket the output file is transferred to. workflow variable attribute (input syntax into workflow) google bucket path consensus this.consensus gs://{user_defined_gcp_bucket}/assemblies/ covhist_out this.coverage_hist gs://{user_defined_gcp_bucket}/bam_stats/ cov_out this.coverage_out gs://{user_defined_gcp_bucket}/bam_stats/ fastqc_clean1_html this.fastq_clean1_html gs://{user_defined_gcp_bucket}/fastqc/ fastqc_clean1_zip this.fastqc_clean1_zip gs://{user_defined_gcp_bucket}/fastqc/ fastqc_clean2_html this.fastqc_clean2_html gs://{user_defined_gcp_bucket}/fastqc/ fastqc_clean2_zip this.fastqc_clean2_zip gs://{user_defined_gcp_bucket}/fastqc/ filtered_reads_1 this.filtered_reads_1 gs://{user_defined_gcp_bucket}/seqyclean/ filtered_reads_2 this.filtered_reads_2 gs://{user_defined_gcp_bucket}/seqclean/ flagstat_out this.flagstat_out gs://{user_defined_gcp_bucket}/bamstats/ out_dir this.out_dir N/A renamed_consensus this.renamed_consensus gs://{user_defined_gcp_bucket}/assemblies/ trimsort_bam this.trimsort_bam gs://{user_defined_gcp_bucket}/alignments/ trimsort_bamindex this.trimsort_bamindex gs://{user_defined_gcp_bucket}/alignments/ variants this.variants gs://{user_defined_gcp_bucket}/variants/","title":"Illumina PE"},{"location":"transfer_workflow/#illumina-se","text":"This workflow is not longer maintained. File: SC2_transfer_illumina_se_assembly.wdl This workflow transfers the output file generated from SC2_illumina_se_assemby to a user specified google bucket. Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. The Google Bucket path describes where in the user google bucket the output file is transferred to. workflow variable attribute (input syntax into workflow) google bucket path adapter_stats this.sample{terra_datatable_name}s.adapter_stats gs://{user_defined_gcp_bucket}/filter_reads/ consensus this.sample{terra_datatable_name}s.consensus gs://{user_defined_gcp_bucket}/assemblies/ covhist_out this.sample{terra_datatable_name}s.coverage_hist gs://{user_defined_gcp_bucket}/bam_stats/ cov_out this.sample{terra_datatable_name}s.coverage_out gs://{user_defined_gcp_bucket}/bam_stats/ fastqc_clean_html this.sample{terra_datatable_name}s.fastq_clean_html gs://{user_defined_gcp_bucket}/fastqc/ fastqc_clean_zip this.sample{terra_datatable_name}s.fastqc_clean_zip gs://{user_defined_gcp_bucket}/fastqc/ filtered_reads this.sample{terra_datatable_name}s.filtered_reads gs://{user_defined_gcp_bucket}/seqyclean/ flagstat_out this.sample{terra_datatable_name}s.flagstat_out gs://{user_defined_gcp_bucket}/bamstats/ out_dir this.sample{terra_datatable_name}s.out_dir N/A PhiX_stats this.sample{terra_datatable_name}s.PhiX_stats gs://{user_defined_gcp_bucket}/filtered_reads/ renamed_consensus this.sample{terra_datatable_name}s.renmaed_consensus gs://{user_defined_gcp_bucket}/assemblies/ trimsort_bam this.sample{terra_datatable_name}s.trimsort_bam gs://{user_defined_gcp_bucket}/alignments/ trimsort_bamindex this.sample{terra_datatable_name}s.trimsort_bamindex gs://{user_defined_gcp_bucket}/alignments/ variants this.sample{terra_datatable_name}s.variants gs://{user_defined_gcp_bucket}/variants/","title":"Illumina SE"},{"location":"transfer_workflow/#oxford-nanopore-technologies-ont","text":"File: SC2_transfer_ont_assembly.wdl This workflow transfers the output file generated from SC2_ont_assemby to a user specified google bucket. Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. The Google Bucket path describes where in the user google bucket the output file is transferred to. workflow variable attribute (input syntax into workflow) google bucket path when transfered covhist_out this.coverage_hist gs://{user_defined_gcp_bucket}/bam_stats/ cov_out this.coverage_out gs://{user_defined_gcp_bucket}/bam_stats/ filtered_fastq this.filtered_fastq gs://{user_defined_gcp_bucket}/filtered_fastq/ flagstat_out this.flagstat_out gs://{user_defined_gcp_bucket}/bamstats/ out_dir this.out_dir N/A renamed_consensus this.renmaed_consensus gs://{user_defined_gcp_bucket}/assemblies/ samstats_out this.samstats_out gs://{user_defined_gcp_bucket}/bam_stats/ scaffold_consensus this.scaffold_consensus gs://{user_defined_gcp_bucket}/assemblies/ trimsort_bam this.trimsort_bam gs://{user_defined_gcp_bucket}/alignments/ variants this.variants gs://{user_defined_gcp_bucket}/variants/","title":"Oxford Nanopore Technologies (ONT)"},{"location":"wastewater_workflow/","text":"Wastewater Workflow This workflow is undergoing updates. For wastewater samples, use one of the assembly workflows above to generate coordinate sorted and primer trimmed bam files. Then see below for how we use freyja for performing variant calling, lineage demixing to generate summary files for wastewater whole genome sequencing of SARS-CoV-2, as well as custom code to summarize VOC-associated constellations of mutations. The workflow accepts \"sample_set\" as the root entity type and uses the data table from any of the three assembly workflows. All three assembly workflows (illumina pe, illumina se and ont) are compatible with this workflow. Briefly the workflow performs the following: Add read groups to the bam files using samtools Use Freya variants to perform variant calling using freya and generate a depths filter_reads Run Freya demix to perform lineage deconvolution and to estimate lineage abundances Pull out a set of curated VOC-associated mutations from the variants file generated by Freya Generate a summary of constellations of VOC-associated mutations transfer the outputs to a user-defined google bucket Inputs Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"\"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. These variables were either in the original terra datatable as inputs for the assembly workflow (see referece based assembly workflow inputs sections above for more details) or added as outputs during the assemlby workflow (see reference based assembly workflow outputs sections for more details). The \"workspace.\" syntax refers Terra to pull the variable from the terra workspace data. Workspace data is describe in the Getting Started drop down menu above. workflow variable attribute (input syntax into workflow) covid_genome workspace.covid_genome out_dir 'gs://covid_terra/{seq_run}/terra_outputs' (must be written as a string in quotes-- we have not updated the workflow to accept the out_dir column from the terra datatable) sample_id this.sample{terra_datatable_name}s.sample{terra_datatable_name}_id trimsort_bam this.sample{terra_datatable_name}s.trimsort_bam voc_annotations workspace.voc_annotations voc_bed workspace.voc_bed Outputs This workflow generates several output files which are transfered to the user defined google bucket as defined by a string (e.g. \"gs://covid_terra/NEXSEQ_101/terra_outputs\"). The table below details each output. For more details regarding the values in each column for the outputs see either the software readmes or the readme for the specific python script as listed in the description. output variable name file_name description google bucket path addrg_bam {sample_name}_addRG.bam ??? N/A variants {smaple_name}_variants.tsv generated for each sample; output from freyja demix gs://{user_defined_gcp_bucket}/waste_water_variant_calling/freyja/ depth {sample_name}_depth.tsv generated for each sample; output from freyja variants gs://{user_defined_gcp_bucket}/waste_water_variant_calling/freyja/ demix {sample_name}_demixed.tsv generated for each sample; output from freyja demix gs://{user_defined_gcp_bucket}/waste_water_variant_calling/freyja/ fill_NA_tsv {sample_name}_voc_fill_NA.tsv N/A reformatted_tsv {sample_name}_voc_reformat.tsv generated for each sample N/A sample_voc_tsv_summary {sample_name}_voc_mutations_forsummary.tsv generated for each sample gs://{user_defined_gcp_bucket}/waste_water_variant_calling/sample_variants/ sample_voc_tsv_counts {sample_name}_voc_mutations_counts.tsv generated for each sample gs://{user_defined_gcp_bucket}/waste_water_variant_calling/sample_variants/ demix_reformatted {sample_id}_demixed_reformatted.tsv generated for each sample N/A voc_summary_temp voc_mutations_summary_temp.tsv N/A voc_counts voc_mutations_counts.tsv gs://{user_defined_gcp_bucket}/waste_water_variant_calling/ voc_summary voc_mutations_summary.tsv gs://{user_defined_gcp_bucket}/waste_water_variant_calling/ demix_summary lineage_abundances.tsv gs://{user_defined_gcp_bucket}/waste_water_variant_calling/ transfer_date N/A date the workflow was run N/A","title":"Wastewater Workflow"},{"location":"wastewater_workflow/#wastewater-workflow","text":"This workflow is undergoing updates. For wastewater samples, use one of the assembly workflows above to generate coordinate sorted and primer trimmed bam files. Then see below for how we use freyja for performing variant calling, lineage demixing to generate summary files for wastewater whole genome sequencing of SARS-CoV-2, as well as custom code to summarize VOC-associated constellations of mutations. The workflow accepts \"sample_set\" as the root entity type and uses the data table from any of the three assembly workflows. All three assembly workflows (illumina pe, illumina se and ont) are compatible with this workflow. Briefly the workflow performs the following: Add read groups to the bam files using samtools Use Freya variants to perform variant calling using freya and generate a depths filter_reads Run Freya demix to perform lineage deconvolution and to estimate lineage abundances Pull out a set of curated VOC-associated mutations from the variants file generated by Freya Generate a summary of constellations of VOC-associated mutations transfer the outputs to a user-defined google bucket","title":"Wastewater Workflow"},{"location":"wastewater_workflow/#inputs","text":"Below is a summary of the workflow input variables along with the syntax used for the attribute column when setting up the workflow to run on Terra.bio. For the attributes, the \"\"this.sample{terra_datatable_name}s.\" syntax refers Terra to pull the variable from the terra datatable as used for sample sets. These variables were either in the original terra datatable as inputs for the assembly workflow (see referece based assembly workflow inputs sections above for more details) or added as outputs during the assemlby workflow (see reference based assembly workflow outputs sections for more details). The \"workspace.\" syntax refers Terra to pull the variable from the terra workspace data. Workspace data is describe in the Getting Started drop down menu above. workflow variable attribute (input syntax into workflow) covid_genome workspace.covid_genome out_dir 'gs://covid_terra/{seq_run}/terra_outputs' (must be written as a string in quotes-- we have not updated the workflow to accept the out_dir column from the terra datatable) sample_id this.sample{terra_datatable_name}s.sample{terra_datatable_name}_id trimsort_bam this.sample{terra_datatable_name}s.trimsort_bam voc_annotations workspace.voc_annotations voc_bed workspace.voc_bed","title":"Inputs"},{"location":"wastewater_workflow/#outputs","text":"This workflow generates several output files which are transfered to the user defined google bucket as defined by a string (e.g. \"gs://covid_terra/NEXSEQ_101/terra_outputs\"). The table below details each output. For more details regarding the values in each column for the outputs see either the software readmes or the readme for the specific python script as listed in the description. output variable name file_name description google bucket path addrg_bam {sample_name}_addRG.bam ??? N/A variants {smaple_name}_variants.tsv generated for each sample; output from freyja demix gs://{user_defined_gcp_bucket}/waste_water_variant_calling/freyja/ depth {sample_name}_depth.tsv generated for each sample; output from freyja variants gs://{user_defined_gcp_bucket}/waste_water_variant_calling/freyja/ demix {sample_name}_demixed.tsv generated for each sample; output from freyja demix gs://{user_defined_gcp_bucket}/waste_water_variant_calling/freyja/ fill_NA_tsv {sample_name}_voc_fill_NA.tsv N/A reformatted_tsv {sample_name}_voc_reformat.tsv generated for each sample N/A sample_voc_tsv_summary {sample_name}_voc_mutations_forsummary.tsv generated for each sample gs://{user_defined_gcp_bucket}/waste_water_variant_calling/sample_variants/ sample_voc_tsv_counts {sample_name}_voc_mutations_counts.tsv generated for each sample gs://{user_defined_gcp_bucket}/waste_water_variant_calling/sample_variants/ demix_reformatted {sample_id}_demixed_reformatted.tsv generated for each sample N/A voc_summary_temp voc_mutations_summary_temp.tsv N/A voc_counts voc_mutations_counts.tsv gs://{user_defined_gcp_bucket}/waste_water_variant_calling/ voc_summary voc_mutations_summary.tsv gs://{user_defined_gcp_bucket}/waste_water_variant_calling/ demix_summary lineage_abundances.tsv gs://{user_defined_gcp_bucket}/waste_water_variant_calling/ transfer_date N/A date the workflow was run N/A","title":"Outputs"}]}